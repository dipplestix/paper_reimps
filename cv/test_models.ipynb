{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(epoch, train_loss, val_loss, val_top1_error, val_top5_error):\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_top1_error\": val_top1_error,\n",
    "        \"val_top5_error\": val_top5_error\n",
    "    })\n",
    "\n",
    "def validate_model(model, valloader, criterion, device):\n",
    "    model.eval()\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Top-1 and Top-5 accuracy\n",
    "            _, predicted = outputs.topk(5, 1, largest=True, sorted=True)\n",
    "            total += labels.size(0)\n",
    "            correct_top1 += (predicted[:, 0] == labels).sum().item()\n",
    "            correct_top5 += labels.unsqueeze(1).eq(predicted).sum().item()\n",
    "\n",
    "    val_loss /= len(valloader)\n",
    "    top1_error = 100. * (1 - correct_top1 / total)\n",
    "    top5_error = 100. * (1 - correct_top5 / total)\n",
    "    \n",
    "    return val_loss, top1_error, top5_error\n",
    "\n",
    "def train_model(model, trainloader, valloader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, top1_error, top5_error = validate_model(model, valloader, criterion, device)\n",
    "\n",
    "        # Log metrics\n",
    "        log_metrics(epoch, epoch_loss, val_loss, top1_error, top5_error)\n",
    "        if epoch == 0:\n",
    "            num_params = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"Number of parameters: {num_params}\")\n",
    "            print(f\"Training model on {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {epoch_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Top-1 Error: {top1_error:.2f}%, '\n",
    "              f'Val Top-5 Error: {top5_error:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                break\n",
    "\n",
    "    print('Finished Training')\n",
    "    wandb.finish()  # Disconnect from wandb at the end of training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdieplstks\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dipplestix/Projects/paper_reimps/cv/wandb/run-20241025_000438-ntx9v4u9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dieplstks/cifar10-alexnet/runs/ntx9v4u9' target=\"_blank\">alexnet-baseline</a></strong> to <a href='https://wandb.ai/dieplstks/cifar10-alexnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dieplstks/cifar10-alexnet' target=\"_blank\">https://wandb.ai/dieplstks/cifar10-alexnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dieplstks/cifar10-alexnet/runs/ntx9v4u9' target=\"_blank\">https://wandb.ai/dieplstks/cifar10-alexnet/runs/ntx9v4u9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 46787978\n",
      "Training model on NVIDIA GeForce RTX 4090\n",
      "Epoch [1/50], Train Loss: 1.8248, Val Loss: 1.4903, Val Top-1 Error: 54.90%, Val Top-5 Error: 8.24%\n",
      "Epoch [2/50], Train Loss: 1.4263, Val Loss: 1.3505, Val Top-1 Error: 49.52%, Val Top-5 Error: 6.43%\n",
      "Epoch [3/50], Train Loss: 1.2680, Val Loss: 1.2491, Val Top-1 Error: 44.51%, Val Top-5 Error: 5.79%\n",
      "Epoch [4/50], Train Loss: 1.1521, Val Loss: 1.1281, Val Top-1 Error: 40.04%, Val Top-5 Error: 4.42%\n",
      "Epoch [5/50], Train Loss: 1.0693, Val Loss: 1.0958, Val Top-1 Error: 38.35%, Val Top-5 Error: 4.09%\n",
      "Epoch [6/50], Train Loss: 1.0020, Val Loss: 1.0396, Val Top-1 Error: 36.21%, Val Top-5 Error: 3.92%\n",
      "Epoch [7/50], Train Loss: 0.9354, Val Loss: 1.0069, Val Top-1 Error: 34.98%, Val Top-5 Error: 3.45%\n",
      "Epoch [8/50], Train Loss: 0.8777, Val Loss: 1.0174, Val Top-1 Error: 35.15%, Val Top-5 Error: 3.63%\n",
      "Epoch [9/50], Train Loss: 0.8202, Val Loss: 1.0063, Val Top-1 Error: 34.24%, Val Top-5 Error: 3.76%\n",
      "Epoch [10/50], Train Loss: 0.7710, Val Loss: 1.0048, Val Top-1 Error: 34.30%, Val Top-5 Error: 3.55%\n",
      "Epoch [11/50], Train Loss: 0.7196, Val Loss: 0.9772, Val Top-1 Error: 33.06%, Val Top-5 Error: 3.33%\n",
      "Epoch [12/50], Train Loss: 0.6764, Val Loss: 0.9563, Val Top-1 Error: 31.94%, Val Top-5 Error: 3.33%\n",
      "Epoch [13/50], Train Loss: 0.6184, Val Loss: 0.9820, Val Top-1 Error: 32.63%, Val Top-5 Error: 3.10%\n",
      "Epoch [14/50], Train Loss: 0.5883, Val Loss: 1.0132, Val Top-1 Error: 32.92%, Val Top-5 Error: 3.30%\n",
      "Epoch [15/50], Train Loss: 0.5425, Val Loss: 1.0108, Val Top-1 Error: 32.46%, Val Top-5 Error: 3.04%\n",
      "Epoch [16/50], Train Loss: 0.4882, Val Loss: 1.0262, Val Top-1 Error: 32.48%, Val Top-5 Error: 3.46%\n",
      "Epoch [17/50], Train Loss: 0.4740, Val Loss: 1.0638, Val Top-1 Error: 32.88%, Val Top-5 Error: 3.35%\n",
      "Early stopping after 17 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▅▃▃▂▂▂▂▂▁▁▁▂▂▂▂</td></tr><tr><td>val_top1_error</td><td>█▆▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_top5_error</td><td>█▆▅▃▂▂▂▂▂▂▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>train_loss</td><td>0.47404</td></tr><tr><td>val_loss</td><td>1.06381</td></tr><tr><td>val_top1_error</td><td>32.88</td></tr><tr><td>val_top5_error</td><td>3.35</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">alexnet-baseline</strong> at: <a href='https://wandb.ai/dieplstks/cifar10-alexnet/runs/ntx9v4u9' target=\"_blank\">https://wandb.ai/dieplstks/cifar10-alexnet/runs/ntx9v4u9</a><br/> View project at: <a href='https://wandb.ai/dieplstks/cifar10-alexnet' target=\"_blank\">https://wandb.ai/dieplstks/cifar10-alexnet</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241025_000438-ntx9v4u9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from krizhevsky2012imagenet.alexnet import AlexNet\n",
    "\n",
    "wandb.init(project=\"cifar10-alexnet\", name=\"alexnet-baseline\")\n",
    "\n",
    "# Log hyperparameters\n",
    "wandb.config.update({\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"model\": \"AlexNet\"\n",
    "})\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = AlexNet(out_dim=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, trainloader, valloader, criterion, optimizer, device, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dipplestix/Projects/paper_reimps/cv/wandb/run-20241025_001028-i49olmt6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dieplstks/cifar10-alexnet/runs/i49olmt6' target=\"_blank\">alexnet-se</a></strong> to <a href='https://wandb.ai/dieplstks/cifar10-alexnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dieplstks/cifar10-alexnet' target=\"_blank\">https://wandb.ai/dieplstks/cifar10-alexnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dieplstks/cifar10-alexnet/runs/i49olmt6' target=\"_blank\">https://wandb.ai/dieplstks/cifar10-alexnet/runs/i49olmt6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 46842378\n",
      "Training model on NVIDIA GeForce RTX 4090\n",
      "Epoch [1/50], Train Loss: 1.8100, Val Loss: 1.4146, Val Top-1 Error: 52.54%, Val Top-5 Error: 6.79%\n",
      "Epoch [2/50], Train Loss: 1.3065, Val Loss: 1.1621, Val Top-1 Error: 41.36%, Val Top-5 Error: 4.98%\n",
      "Epoch [3/50], Train Loss: 1.0663, Val Loss: 0.9810, Val Top-1 Error: 34.77%, Val Top-5 Error: 3.24%\n",
      "Epoch [4/50], Train Loss: 0.9201, Val Loss: 0.8732, Val Top-1 Error: 30.50%, Val Top-5 Error: 2.55%\n",
      "Epoch [5/50], Train Loss: 0.8008, Val Loss: 0.8224, Val Top-1 Error: 28.88%, Val Top-5 Error: 2.37%\n",
      "Epoch [6/50], Train Loss: 0.7014, Val Loss: 0.7907, Val Top-1 Error: 27.02%, Val Top-5 Error: 2.41%\n",
      "Epoch [7/50], Train Loss: 0.6249, Val Loss: 0.7570, Val Top-1 Error: 26.08%, Val Top-5 Error: 2.00%\n",
      "Epoch [8/50], Train Loss: 0.5589, Val Loss: 0.7643, Val Top-1 Error: 25.25%, Val Top-5 Error: 1.94%\n",
      "Epoch [9/50], Train Loss: 0.4868, Val Loss: 0.7595, Val Top-1 Error: 25.33%, Val Top-5 Error: 2.11%\n",
      "Epoch [10/50], Train Loss: 0.4460, Val Loss: 0.7631, Val Top-1 Error: 24.63%, Val Top-5 Error: 2.03%\n",
      "Epoch [11/50], Train Loss: 0.3815, Val Loss: 0.7960, Val Top-1 Error: 24.76%, Val Top-5 Error: 2.01%\n",
      "Epoch [12/50], Train Loss: 0.3420, Val Loss: 0.8485, Val Top-1 Error: 24.99%, Val Top-5 Error: 2.20%\n",
      "Early stopping after 12 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▆▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▂</td></tr><tr><td>val_top1_error</td><td>█▅▄▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_top5_error</td><td>█▅▃▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>train_loss</td><td>0.34203</td></tr><tr><td>val_loss</td><td>0.84849</td></tr><tr><td>val_top1_error</td><td>24.99</td></tr><tr><td>val_top5_error</td><td>2.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">alexnet-se</strong> at: <a href='https://wandb.ai/dieplstks/cifar10-alexnet/runs/i49olmt6' target=\"_blank\">https://wandb.ai/dieplstks/cifar10-alexnet/runs/i49olmt6</a><br/> View project at: <a href='https://wandb.ai/dieplstks/cifar10-alexnet' target=\"_blank\">https://wandb.ai/dieplstks/cifar10-alexnet</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241025_001028-i49olmt6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hu2019squeezeexcitation.alexnet_se import AlexNetSE\n",
    "\n",
    "\n",
    "wandb.init(project=\"cifar10-alexnet\", name=\"alexnet-se\")\n",
    "\n",
    "# Log hyperparameters\n",
    "wandb.config.update({\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 128,\n",
    "    \"model\": \"AlexNetSE\"\n",
    "})\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = AlexNetSE(out_dim=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, trainloader, valloader, criterion, optimizer, device, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
