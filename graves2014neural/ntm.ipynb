{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(nn.Module):\n",
    "    def __init__(self, N, M):\n",
    "        self.memory = torch.zeros(N, M)\n",
    "\n",
    "    \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddressingMechanism:\n",
    "    def __init__(self, w, M, k, beta, g, s, gamma, shifts=[-1, 0, 1]):\n",
    "        self.w = w\n",
    "        self.M = M\n",
    "        self.k = k\n",
    "        self.beta = beta\n",
    "        self.g = g\n",
    "        self.s = s\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.N = self.M.shape[0]\n",
    "        self.shifts = shifts\n",
    "\n",
    "    def content_addressing(self):\n",
    "        sim = F.cosine_similarity(self.k.unsqueeze(1), self.M.unsqueeze(0), dim=-1)\n",
    "        self.wc = F.softmax(self.beta.unsqueeze(1)*sim, dim=0)\n",
    "\n",
    "    def interpolation(self):\n",
    "        self.wg = self.g*self.wc + (1 - self.g)*self.w\n",
    "        \n",
    "\n",
    "    def shift(self):\n",
    "        w_tilde = torch.zeros(self.N)\n",
    "        for i in range(self.N)\n",
    "            for j in range(self.N):\n",
    "                if (i - j) % self.N in self.shifts:\n",
    "                    w_tilde[i] += self.wg[j]*self.s[]\n",
    "\n",
    "    def sharpening(self):\n",
    "        w_tilde = torch.pow(self.w_tilde, self.gamma)\n",
    "        self.wt = w_tilde/w_tilde.sum()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "shifts = torch.tensor([-1, 0, 1])\n",
    "(shifts==0).nonzero(as_tuple=False).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0568)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0568)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = F.cosine_similarity(k, M)\n",
    "print(F.softmax(sim, dim=0)[0])\n",
    "\n",
    "sim_s = F.cosine_similarity(k, M)\n",
    "F.softmax(sim_s, dim=0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0568)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(sim[0])/torch.exp(sim).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8503)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(M[0]*k).sum()/(torch.norm(M[0])*torch.norm(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4707)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(M[0]*k).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_addressing(k, M, beta):\n",
    "    sim = F.cosine_similarity(k, M)\n",
    "    wc = F.softmax(beta*sim, dim=0)\n",
    "    return wc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = torch.zeros(100, 20)\n",
    "for i in range(k.shape[0]):\n",
    "    wc[i] = F.cosine_similarity(k[i], M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc2 = F.cosine_similarity(k.unsqueeze(1), M.unsqueeze(0), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(wc2, dim=0).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 5)\n",
    "b = torch.tensor([1, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8539, 0.9458, 0.2876, 0.7212, 0.5269],\n",
       "        [4.7286, 4.5559, 2.6117, 2.0051, 1.5541]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(1)*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8539, 0.9458, 0.2876, 0.7212, 0.5269],\n",
       "        [0.7881, 0.7593, 0.4353, 0.3342, 0.2590]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.rand(100, 10)\n",
    "M = torch.rand(100, 20, 10)\n",
    "beta = torch.rand(100)\n",
    "g = torch.rand(100)\n",
    "w = torch.rand(100, 20)\n",
    "gamma = torch.rand(100)\n",
    "s = torch.rand(100, 3)\n",
    "shifts = torch.tensor([-1, 0, 1])\n",
    "\n",
    "\n",
    "def content_addressing(k, M, beta):\n",
    "    sim = F.cosine_similarity(k.unsqueeze(1), M.unsqueeze(0), dim=-1)\n",
    "    wc = F.softmax(beta.unsqueeze(1)*sim, dim=0)\n",
    "    wc = wc.squeeze(0)\n",
    "    return wc\n",
    "\n",
    "def interpolation(g, wc, w):\n",
    "    g = g.unsqueeze(1)\n",
    "    wg = g*wc + (1 - g)*w\n",
    "    return wg\n",
    "\n",
    "def shift(wg, s, shifts):\n",
    "    N = wg.shape[1]\n",
    "    w_tilde = torch.zeros_like(wg)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if (i - j) % N in shifts:\n",
    "                w_tilde[:, i] = wg[:, j]*s[:, (shifts==((i - j) % N)).nonzero().squeeze()]\n",
    "    return w_tilde\n",
    "\n",
    "\n",
    "def sharpening(w_tilde, gamma):\n",
    "    w_tilde = torch.pow(w_tilde, gamma.unsqueeze(1))\n",
    "    wt = w_tilde/w_tilde.sum()   \n",
    "    return wt \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc = content_addressing(k, M, beta)\n",
    "wg = interpolation(g, wc, w)\n",
    "w_tilde = shift(wg, s, shifts)\n",
    "wt = sharpening(w_tilde, gamma)\n",
    "\n",
    "torch.einsum('ij, ijk->ik', w, M).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(shifts == 0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 20, 10])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
