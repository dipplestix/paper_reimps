{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ce34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b45161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 50_000\n",
    "MIN_REPLAY_SIZE = 1_000\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 500  \n",
    "\n",
    "TARGET_UPDATE_FREQ = 1000 \n",
    "TRAIN_FREQ = 4\n",
    "MAX_STEPS = 200_000\n",
    "EVAL_EPISODES = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceff3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = collections.namedtuple(\n",
    "    \"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        batch = Transition(*zip(*batch))  # transpose\n",
    "        # Convert to tensors\n",
    "        states = torch.as_tensor(np.array(batch.state), dtype=torch.float32, device=DEVICE)\n",
    "        actions = torch.as_tensor(batch.action, dtype=torch.int64, device=DEVICE).unsqueeze(-1)\n",
    "        rewards = torch.as_tensor(batch.reward, dtype=torch.float32, device=DEVICE).unsqueeze(-1)\n",
    "        next_states = torch.as_tensor(np.array(batch.next_state), dtype=torch.float32, device=DEVICE)\n",
    "        dones = torch.as_tensor(batch.done, dtype=torch.float32, device=DEVICE).unsqueeze(-1)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3a3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EpsilonScheduler:\n",
    "    eps_start: float = EPS_START\n",
    "    eps_end: float = EPS_END\n",
    "    eps_decay: int = EPS_DECAY\n",
    "\n",
    "    def value(self, step: int) -> float:\n",
    "        # Exponential decay\n",
    "        return self.eps_end + (self.eps_start - self.eps_end) * np.exp(-1.0 * step / self.eps_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25e26217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_dim: int, n_actions: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8dfce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0086954 ,  0.01555407,  0.02404243, -0.01035482], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "obs, _ = env.reset()\n",
    "\n",
    "\n",
    "print(env.observation_space.shape)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02db1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0e6f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = torch.tensor([[1], [0], [0], [1]])\n",
    "qs = torch.rand(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7a0df57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0436],\n",
       "        [0.3663],\n",
       "        [0.6396],\n",
       "        [0.1610]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs.gather(1, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c20f3d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3254, 0.0436],\n",
       "        [0.3663, 0.1533],\n",
       "        [0.6396, 0.0343],\n",
       "        [0.6579, 0.1610]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5f23046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0375, 0.3798],\n",
      "        [0.5424, 0.4772],\n",
      "        [0.3766, 0.0095],\n",
      "        [0.5031, 0.3305]])\n",
      "torch.return_types.max(\n",
      "values=tensor([0.3798, 0.5424, 0.3766, 0.5031]),\n",
      "indices=tensor([1, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "next_qs = torch.rand(4, 2)\n",
    "print(next_qs)\n",
    "print(next_qs.max(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02484956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env_id: str = \"CartPole-v1\"):\n",
    "    env = gym.make(env_id)\n",
    "    eval_env = gym.make(env_id)\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    q_net = DQN(obs_dim, n_actions).to(DEVICE)\n",
    "    target_net = DQN(obs_dim, n_actions).to(DEVICE)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=LR)\n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "    eps_sched = EpsilonScheduler()\n",
    "\n",
    "    print(\"Filling replay buffer with random policy...\")\n",
    "    while len(replay_buffer) < MIN_REPLAY_SIZE:\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done and len(replay_buffer) < MIN_REPLAY_SIZE:\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            replay_buffer.push(obs, action, reward, next_obs, done)\n",
    "            obs = next_obs\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    obs, _ = env.reset()\n",
    "    episode_reward = 0.0\n",
    "    episode = 0\n",
    "\n",
    "    total_steps = 0\n",
    "    while total_steps < MAX_STEPS:\n",
    "        eps = eps_sched.value(total_steps)\n",
    "        if random.random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "                q_values = q_net(obs_tensor)\n",
    "                action = int(torch.argmax(q_values, dim=1).item())\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        replay_buffer.push(obs, action, reward, next_obs, done)\n",
    "\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "        total_steps += 1\n",
    "\n",
    "        if total_steps % TRAIN_FREQ == 0:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "            # Current Q(s, a)\n",
    "            q_values = q_net(states).gather(1, actions)\n",
    "\n",
    "            # Target Q\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_net(next_states).max(dim=1, keepdim=True)[0]\n",
    "                targets = rewards + GAMMA * (1 - dones) * next_q_values\n",
    "\n",
    "            loss = nn.functional.mse_loss(q_values, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(q_net.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Target network update\n",
    "        if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "            target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "        # End of episode\n",
    "        if done:\n",
    "            episode += 1\n",
    "            print(f\"Episode {episode:4d} | Steps {total_steps:7d} | \"\n",
    "                  f\"Return {episode_reward:6.1f} | eps={eps:.3f}\")\n",
    "            obs, _ = env.reset()\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # -----------------------\n",
    "    # Evaluation\n",
    "    # -----------------------\n",
    "    print(\"\\nEvaluating greedy policy...\")\n",
    "    returns = []\n",
    "    for ep in range(EVAL_EPISODES):\n",
    "        obs, _ = eval_env.reset()\n",
    "        done = False\n",
    "        ep_ret = 0.0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "                q_values = q_net(obs_tensor)\n",
    "                action = int(torch.argmax(q_values, dim=1).item())\n",
    "            obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            ep_ret += reward\n",
    "        returns.append(ep_ret)\n",
    "        print(f\"Eval episode {ep+1}: return = {ep_ret}\")\n",
    "\n",
    "    print(f\"\\nAverage return over {EVAL_EPISODES} eval episodes: {np.mean(returns):.2f}\")\n",
    "    env.close()\n",
    "    eval_env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-solving",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
